{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ac7be50dc8e203",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2afb0a46c1f8e",
   "metadata": {},
   "source": [
    "### Part 00 - Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Feature Scaling is Important in CV\n",
    "# and Pre Processing is Done Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa36b60d7e81f",
   "metadata": {},
   "source": [
    "## Part 01 : Building Convolutional Neural Network\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **`Sequential`**: Initializes the neural network model.\n",
    "- **`Conv2D`**: Adds convolutional layers for feature extraction.\n",
    "- **`MaxPooling2D`**: Adds pooling layers to reduce spatial dimensions and computational complexity.\n",
    "- **`Flatten`**: Converts multi-dimensional data to 1D vectors.\n",
    "- **`Dense`**: Adds fully connected layers for classification or regression.\n",
    "- **`Dropout`**: Prevents overfitting by randomly setting a fraction of input units to 0 during training.\n",
    "\n",
    "These components work together to build a CNN that can learn and classify complex patterns in image data."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ceec3c83c994df4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:47:54.241425Z",
     "start_time": "2024-07-29T05:47:43.505370Z"
    }
   },
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# 2d because images are 2d"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 10:47:45.233797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4e8128009118e1ab",
   "metadata": {},
   "source": [
    "### Initializing CNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "225aae506d873c4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:04.467496Z",
     "start_time": "2024-07-29T05:48:04.314626Z"
    }
   },
   "source": "classifier=Sequential() #  Initialized CNN\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 10:48:04.352056: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "1788776deab593ba",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Initialization**:\n",
    "    ```python\n",
    "    classifier = Sequential()\n",
    "    ```\n",
    "    - This initializes a sequential model, which is a linear stack of layers.\n",
    "\n",
    "2. **First Convolutional Layer**:\n",
    "    ```python\n",
    "    classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
    "    ```\n",
    "    - **`Conv2D(32, kernel_size=(3, 3))`**: Adds a convolutional layer with 32 filters, each of size 3x3.\n",
    "    - **`activation='relu'`**: Uses the ReLU activation function to introduce non-linearity by setting all negative values to zero.\n",
    "    - **`padding='same'`**: Ensures that the output feature map has the same width and height as the input feature map.\n",
    "    - **`input_shape=(64, 64, 3)`**: Specifies the shape of the input data (64x64 RGB images, where 3 is the number of color channels).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9aa1b4abaade2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:12.090461Z",
     "start_time": "2024-07-29T05:48:11.882619Z"
    }
   },
   "source": [
    "classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
    "\n",
    "\n",
    "\n",
    "# Filter : 32 - No of Filters / No of Feature Maps\n",
    "# Kernal Size : No of Rows , No of Columns\n",
    "# activation : activation fucntion\n",
    "# input_shape (theano) : expected images of format colored = 3d array - Black White 2d arrat chossing 64 because of CPU\n",
    "# activation : relu - so that we dont have -ive pixel value in feature maps -  during convolution we got some -ive pixels in feature map amd need to remove non-linearity"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c40ace88ff549998",
   "metadata": {},
   "source": [
    "## Max-Pooling :\n",
    "### Reducing the size of Feature Map ny taking 2 x 2 and stride of 2 compare and wrtie a max of feture map compared to Feature map\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5f52f47a1106442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:15.043743Z",
     "start_time": "2024-07-29T05:48:15.022352Z"
    }
   },
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2, 2))) # Reduced the size and complexity of Model\n",
    "\n",
    "# pool_size is generally 2 x 2 "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "9de7d841-bbd7-4c4e-9ed2-b5c0ff5071a7",
   "metadata": {},
   "source": [
    "## Adding Second Convolutional Layer\n",
    "### Adding another Convolutional Layer to Get Accuracy of more than 80% on test set  and then max pooling (Form Last Step)\n",
    "#### we will only change input_shape parameter i.e what shape of input it should expect and it will not be images but POOLED Feature maps coming from above steps therefore we will not include it - So You don't need it"
   ]
  },
  {
   "cell_type": "code",
   "id": "41a3b7b1-fe11-49dc-9db3-7ec3a0bf594a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:17.993095Z",
     "start_time": "2024-07-29T05:48:17.925769Z"
    }
   },
   "source": [
    "classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "30392a788c945b41",
   "metadata": {},
   "source": [
    "# Flattening\n",
    "\n",
    "### Flattening:\n",
    " It reshapes the extracted features into a 1D vector, which is necessary for dense layers. This step does not lose information because the features have already been processed by previous layers.\n",
    "### Convolutional Layers:\n",
    " They extract meaningful features from the raw pixel values by applying various filters.\n",
    "\n",
    "### Max Pooling Layers:\n",
    " They reduce the spatial dimensions while retaining the most important information, making the network more efficient and robust. \n",
    " \n",
    "### Remeber : Each Feature Map consist of feature"
   ]
  },
  {
   "cell_type": "code",
   "id": "714c4ee70ba2b852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:21.233324Z",
     "start_time": "2024-07-29T05:48:21.205007Z"
    }
   },
   "source": [
    "classifier.add(Flatten())"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e61d7d0e5c9c4cd3",
   "metadata": {},
   "source": [
    "## Full Connection - Making Classic ANN by Fully Connected ANN\n",
    "\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "#### 1. Hidden Fully Connected Layer\n",
    "```python\n",
    "classifier.add(Dense(output_dim=128, activation='relu'))\n",
    "```\n",
    "\n",
    "- **`Dense` Layer**: This is a fully connected neural network layer. It means that each neuron in this layer receives input from all the neurons in the previous layer.\n",
    "- **`output_dim=128`**: This specifies the number of neurons in the layer, which is 128. This number is somewhat arbitrary but is chosen to be large enough to allow the network to learn complex patterns, yet not too large to prevent overfitting and excessive computational cost.\n",
    "- **`activation='relu'`**: The Rectified Linear Unit (ReLU) activation function is applied. ReLU is a common activation function that introduces non-linearity to the model, allowing it to learn complex patterns. It outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "#### 2. Output Layer\n",
    "```python\n",
    "classifier.add(Dense(output_dim=1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "- **`Dense` Layer**: Again, this is a fully connected neural network layer.\n",
    "- **`output_dim=1`**: This specifies the number of neurons in the layer, which is 1. For a binary classification problem, this single neuron will output a value between 0 and 1, representing the probability of the positive class.\n",
    "- **`activation='sigmoid'`**: The sigmoid activation function is used here. It is suitable for binary classification as it maps the input to a value between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "### Explanation of the `output_dim` and `input_dim`\n",
    "\n",
    "- **`units`**: Refers to the number of neurons in the layer. The term \"output dimension\" can be confusing because it is actually defining the size of the layer itself (i.e., the number of neurons).\n",
    "\n",
    "- **Choosing `output_dim`**: The formula \"number of input nodes * number of output nodes\" is a guideline for the number of neurons to use in a hidden layer. However, this can lead to a very large number of neurons. Instead, a more practical approach is to start with a smaller number of neurons and adjust based on model performance and computational constraints. In this case, 128 is chosen as a reasonable number.\n",
    "\n",
    "- **`input_dim`**: Refers to the number of input features the layer expects. \n",
    "\n",
    "### Why No `input_dim` in the First Line of Code?\n",
    "\n",
    "- **First Layer**: When adding the first layer to the model, you need to specify the `input_dim` (number of input features). However, in your code, it appears that the code shown is not the first layer in the model. Hence, `input_dim` is not specified because the model already knows the input dimensions from the previous layers.\n",
    "  \n",
    "- **Subsequent Layers**: For any layers added after the first layer, the input dimension is inferred from the output of the previous layer, so specifying `input_dim` is unnecessary.\n",
    "\n",
    "### Summary\n",
    "- **`Dense` Layer**: Fully connected layer where each neuron receives input from all neurons in the previous layer.\n",
    "- **`units`**: Specifies the number of neurons in the layer.\n",
    "- **`activation`**: Function applied to the output of each neuron to introduce non-linearity.\n",
    "- **ReLU Activation**: Commonly used in hidden layers to allow the model to learn complex patterns.\n",
    "- **Sigmoid Activation**: Used in the output layer for binary classification, providing a probability score between 0 and 1.\n",
    "- **`input_dim`**: Needed only for the first layer to specify the number of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "47613e1fa80e5298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:25.697942Z",
     "start_time": "2024-07-29T05:48:25.557896Z"
    }
   },
   "source": [
    "# Hidden Fully Connected Layer and Will connect Output layer\n",
    "classifier.add(Dense(units=128, activation='relu')) \n",
    "classifier.add(Dense(units=1, activation='sigmoid')) # output Layer\n",
    "\n",
    "\n",
    "# output_dim : no_of_input_nodes  * no_of_output_nodes i.e but here it will become alot therefore we will choose 128 it should be medium"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "35311058-e33a-4c2b-ae3f-51a57ed59c2a",
   "metadata": {},
   "source": [
    "## Compiling CNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf8b2c3a-1615-4365-9b4c-b87d82707c02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:48:28.925103Z",
     "start_time": "2024-07-29T05:48:28.875126Z"
    }
   },
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d6d70cf3-a423-407b-b259-b4627c1d6e0d",
   "metadata": {},
   "source": [
    "## Fit CNN in Our Images\n",
    "### Before fit we will do image augmentation to pre process image to over fitting\n",
    "#### it is important to get accuracy on both test and train set\n",
    "#### So that it can find co-relation between images in train set and also recognize it in test set\n",
    "#### it will create batches and apply random transformation on that images i.e enrich our dataset with adding more images\n",
    "\n",
    "\n",
    "###### Image augmentation is a technique to artificially expand the size of a training dataset by creating modified versions of images in the dataset. This helps improve the robustness and generalization capability of a neural network model.\n",
    "\n",
    "###### Keras provides a built-in tool for image augmentation called ImageDataGenerator, which can be used to generate augmented images on the fly. The method flow_from_directory is indeed a part of this tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa4c3c-30f0-4c14-a9f8-8fb153113922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Image Data Augmentation and Model Training Documentation\n",
    "\n",
    "### 1. Data Augmentation\n",
    "\n",
    "**Importing Libraries:**\n",
    "\n",
    "```python\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "```\n",
    "\n",
    "**Creating Data Generators:**\n",
    "\n",
    "- **Training Data Generator:**\n",
    "\n",
    "    ```python\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,                # Rescale pixel values from [0, 255] to [0, 1]\n",
    "        shear_range=0.2,               # Apply random shear transformations\n",
    "        zoom_range=0.2,                # Apply random zoom transformations\n",
    "        horizontal_flip=True           # Randomly flip images horizontally\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    - **`rescale=1./255`**: Normalizes pixel values from a range of [0, 255] to [0, 1], which helps in faster convergence.\n",
    "    - **`shear_range=0.2`**: Applies random shear transformations to images, helping the model generalize better.\n",
    "    - **`zoom_range=0.2`**: Randomly zooms into images, adding variability to training data.\n",
    "    - **`horizontal_flip=True`**: Randomly flips images horizontally, increasing the variety in the training data.\n",
    "\n",
    "- **Test Data Generator:**\n",
    "\n",
    "    ```python\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale test/validation data\n",
    "    ```\n",
    "\n",
    "    - **`rescale=1./255`**: Normalizes pixel values for test/validation data to match the scale of training data.\n",
    "\n",
    "### 2. Loading Data\n",
    "\n",
    "- **Training Set:**\n",
    "\n",
    "    ```python\n",
    "    training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',        # Directory with training images\n",
    "        target_size=(64, 64),          # Resize all images to 64x64\n",
    "        batch_size=32,                 # Number of images to be yielded from the generator per batch\n",
    "        class_mode='binary'            # Type of label arrays to be returned ('binary' for binary classification)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    - **`directory='dataset/training_set'`**: Path to the directory containing training images.\n",
    "    - **`target_size=(64, 64)`**: Resizes images to 64x64 pixels.\n",
    "    - **`batch_size=32`**: Number of images in each batch.\n",
    "    - **`class_mode='binary'`**: Specifies that the problem is a binary classification task, where labels are 0 or 1.\n",
    "\n",
    "- **Test/Validation Set:**\n",
    "\n",
    "    ```python\n",
    "    test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',            # Directory with test/validation images\n",
    "        target_size=(64, 64),          # Resize all images to 64x64\n",
    "        batch_size=32,                 # Number of images to be yielded from the generator per batch\n",
    "        class_mode='binary'            # Type of label arrays to be returned ('binary' for binary classification)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    - **`directory='dataset/test_set'`**: Path to the directory containing test/validation images.\n",
    "    - **`target_size=(64, 64)`**: Resizes images to 64x64 pixels.\n",
    "    - **`batch_size=32`**: Number of images in each batch.\n",
    "    - **`class_mode='binary'`**: Specifies that the problem is a binary classification task.\n",
    "\n",
    "### 3. Training the Model\n",
    "\n",
    "**Fitting the Model:**\n",
    "\n",
    "```python\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=8000/32,       # Number of steps per epoch, usually total samples / batch size\n",
    "    epochs=25,                     # Number of epochs to train\n",
    "    validation_data=test_set,      # Data for validation\n",
    "    validation_steps=2000/32       # Number of steps for validation, usually total validation samples / batch size\n",
    ")\n",
    "```\n",
    "\n",
    "- **`steps_per_epoch=8000/32`**: Number of batches to process in each epoch. With 8000 training images and a batch size of 32, this results in 250 steps per epoch.\n",
    "- **`epochs=25`**: Number of epochs to train the model. The model will go through the entire training set 25 times.\n",
    "- **`validation_data=test_set`**: Data to evaluate the model's performance on unseen data during training.\n",
    "- **`validation_steps=2000/32`**: Number of batches to process for validation in each epoch. Adjust based on the size of your validation set.\n",
    "\n",
    "---\n",
    "\n",
    "This documentation covers how to set up data augmentation, load data from directories, and train your model using the `ImageDataGenerator` class in Keras."
   ]
  },
  {
   "cell_type": "code",
   "id": "dae92017-d8ff-4b87-b8ff-220acab06771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T05:50:07.446822Z",
     "start_time": "2024-07-29T05:48:35.893164Z"
    }
   },
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    shear_range=0.2,               # Apply random shear transformations\n",
    "    zoom_range=0.2,                # Apply random zoom transformations\n",
    "    horizontal_flip=True           # Randomly flip images horizontally\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale test/validation data\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',        # Directory with training images\n",
    "    target_size=(64, 64),          # Resize all images to 64x64 ; Size of Image Expected\n",
    "    batch_size=32,                 # Number of images to be yielded from the generator per batch\n",
    "    class_mode='binary'            # Type of label arrays to be returned ('binary' for binary classification) or have more categories\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',            # Directory with test/validation images\n",
    "    target_size=(64, 64),          # Resize all images to 64x64\n",
    "    batch_size=32,                 # Number of images to be yielded from the generator per batch\n",
    "    class_mode='binary'            # Type of label arrays to be returned ('binary' for binary classification) \n",
    ")\n",
    "\n",
    "\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=8000/32,       # Number of steps per epoch, usually total samples / batch size\n",
    "    epochs=25,                     # Number of epochs to train\n",
    "    validation_data=test_set,      # Data for validation\n",
    "    validation_steps=2000/32       # Number of steps for validation, usually total validation samples / batch size\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 10:48:38.267089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/250 [============>.................] - ETA: 1:37 - loss: 0.7031 - accuracy: 0.5230"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 28\u001B[0m\n\u001B[1;32m     13\u001B[0m training_set \u001B[38;5;241m=\u001B[39m train_datagen\u001B[38;5;241m.\u001B[39mflow_from_directory(\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset/training_set\u001B[39m\u001B[38;5;124m'\u001B[39m,        \u001B[38;5;66;03m# Directory with training images\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     target_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m),          \u001B[38;5;66;03m# Resize all images to 64x64 ; Size of Image Expected\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,                 \u001B[38;5;66;03m# Number of images to be yielded from the generator per batch\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     class_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m            \u001B[38;5;66;03m# Type of label arrays to be returned ('binary' for binary classification) or have more categories\u001B[39;00m\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     20\u001B[0m test_set \u001B[38;5;241m=\u001B[39m test_datagen\u001B[38;5;241m.\u001B[39mflow_from_directory(\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset/test_set\u001B[39m\u001B[38;5;124m'\u001B[39m,            \u001B[38;5;66;03m# Directory with test/validation images\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     target_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m),          \u001B[38;5;66;03m# Resize all images to 64x64\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,                 \u001B[38;5;66;03m# Number of images to be yielded from the generator per batch\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     class_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m            \u001B[38;5;66;03m# Type of label arrays to be returned ('binary' for binary classification) \u001B[39;00m\n\u001B[1;32m     25\u001B[0m )\n\u001B[0;32m---> 28\u001B[0m classifier\u001B[38;5;241m.\u001B[39mfit(\n\u001B[1;32m     29\u001B[0m     training_set,\n\u001B[1;32m     30\u001B[0m     steps_per_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m32\u001B[39m,       \u001B[38;5;66;03m# Number of steps per epoch, usually total samples / batch size\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,                     \u001B[38;5;66;03m# Number of epochs to train\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39mtest_set,      \u001B[38;5;66;03m# Data for validation\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     validation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2000\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m32\u001B[39m       \u001B[38;5;66;03m# Number of steps for validation, usually total validation samples / batch size\u001B[39;00m\n\u001B[1;32m     34\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/training.py:1685\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1677\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1678\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1679\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1682\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1683\u001B[0m ):\n\u001B[1;32m   1684\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1685\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1687\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    891\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 894\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    896\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    897\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    923\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    924\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    925\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 926\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_no_variable_creation_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    928\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    929\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    930\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    141\u001B[0m   (concrete_function,\n\u001B[1;32m    142\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\u001B[38;5;241m.\u001B[39m_call_flat(\n\u001B[1;32m    144\u001B[0m     filtered_flat_args, captured_inputs\u001B[38;5;241m=\u001B[39mconcrete_function\u001B[38;5;241m.\u001B[39mcaptured_inputs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1753\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1755\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1756\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1757\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inference_function\u001B[38;5;241m.\u001B[39mcall(\n\u001B[1;32m   1758\u001B[0m       ctx, args, cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager))\n\u001B[1;32m   1759\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1760\u001B[0m     args,\n\u001B[1;32m   1761\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1762\u001B[0m     executing_eagerly)\n\u001B[1;32m   1763\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    380\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 381\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute(\n\u001B[1;32m    382\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    383\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m    384\u001B[0m         inputs\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m    385\u001B[0m         attrs\u001B[38;5;241m=\u001B[39mattrs,\n\u001B[1;32m    386\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx)\n\u001B[1;32m    387\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    388\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    390\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    394\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     53\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "c2dd501b-ce76-49df-a2fa-dde9794caf5b",
   "metadata": {},
   "source": [
    "## Increasing the Accuray to 80 Percent under test set as we are getting 75 on test and 84 on train\n",
    "\n",
    "### Two Options\n",
    "\n",
    "#### Add another Convolutional Layer (Best)\n",
    "#### Add another Fully Connected Layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
